{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "In this exercise we will learn how to perform text classification. For instance we will perform sentiment analisys on movie reviews. You have a folder *../data/text_polarity/* with two files one with positive and one negative with negative reviews.\n",
    "\n",
    "First we will import some packages that we need for our task and here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's do we have to do first?\n",
    "Get in contact with the data. How does it look like? Do we need to delete something? Take a look at the data, think about the polarity, do you agree with the sentiment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = open(\"../data/text_polarity/positive_pl.txt\", \"r\")\n",
    "\n",
    "for line in pos_data:\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "Prepare the data. \n",
    "\n",
    "**How?** \n",
    "\n",
    "Could you see how many reviews do we have in each file? \n",
    "The idea is that you read those files merge them into one and create labels for them.\n",
    "\n",
    "\n",
    "\n",
    "***Hint:***\n",
    "If you open a file using readlines, it would return a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = open(\"../data/text_polarity/positive_pl.txt\", \"r\").readlines()\n",
    "neg_data = open(\"../data/text_polarity/negative_pl.txt\", \"r\").readlines()\n",
    "\n",
    "print(len(pos_data))\n",
    "print(len(neg_data))\n",
    "\n",
    "data = pos_data + neg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the labels. You might remember that we already imported pandas and numpy. \n",
    "\n",
    "***Hint 1:***\n",
    "Remember to use numerical values as labels, for example positive 0 and negative 1. Or viceversa.\n",
    "\n",
    "**Hint 2:**\n",
    "The numpy function full creates a vector with a desired shape and fullfils it with a desired value. Create your vectors and concatenate them into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.full(len(pos_data), 1)\n",
    "neg_labels = np.full(len(neg_data), 0)\n",
    "\n",
    "labels = np.concatenate((pos_labels, neg_labels))\n",
    "\n",
    "labels[0]\n",
    "print(\"Labels\", len(labels))\n",
    "print(\"Data\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the data into train and test sets and shuffle, so that our model does not learn from data order. Scikit Learn has a function that performs those splits for you. \n",
    "\n",
    "You can check `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to create store your data in X_train, y_train, X_test, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=1234, shuffle=True)\n",
    "print(X_train[15])\n",
    "print(y_train[15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally models do not process text but a numerical version of it. Therefore we need to vectorize our data. There are several ways of doing it, you might remember that from our previous presentation. Here are a couple of them:\n",
    "\n",
    "**Bag of words**: \n",
    "Let's imagine that we have four documents:\n",
    "\n",
    " 1. I like to eat an apple every day\n",
    " 2. I like to eat some fruits every day\n",
    " 3. I do not like to eat fruits\n",
    " 4. I like to cook with fruits every day\n",
    " \n",
    "What about if we list all words from our documents, how would you do it?\n",
    "You can check the string function `split` and the method `sorted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"I like to eat apples \" \\\n",
    "            \"I like to eat fruits \" \\\n",
    "            \"I do not like apples \" \\\n",
    "            \"I like to cook every day\"\n",
    "\n",
    "words = documents.split(\" \")\n",
    "word_list = list(set(words))\n",
    "print(sorted(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the list, we can now write manually our bag of words for each document:\n",
    "\n",
    "1. [1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
    "2. [1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n",
    "3. [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
    "4. [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "And tada!! This are the vectors that we would feed in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-Idf**:\n",
    "Another way of vectorizing the data could be calculating Tf-Idf for each document.\n",
    "\n",
    "- Tf: Term frequency: $\\frac{freq(word)}{\\# words \\in doc} $\n",
    "- Idf: Inverse document frequency: $\\log\\frac{|D|}{\\# d : word \\in doc}$\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Tf-Idf(I) = $\\frac{1}{5}*\\log(\\frac{4}{4}) = 0$\n",
    "- Tf-Idf(apples) = $\\frac{1}{5}*\\log(\\frac{4}{2}) = 0.2*0.301 = 0.0602$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we do not want to calculate this for every word in our corpus. Therefore, in order to vectorize our data, we can use the `TfidfVectorizer` function from SkLearn and test our model on the preprocessed data. The resulting vector from this operation is the input to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=5)\n",
    "trainMatrixTfidf = vectorizer.fit_transform(X_train)\n",
    "trainInput = trainMatrixTfidf\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(trainInput, y_train)\n",
    "testMatrixTfidf = vectorizer.transform(X_test)\n",
    "test = clf.predict(testMatrixTfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step is to calculate accuracy, which is relevant in this case, since we already know that this corpus is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = np.sum(np.equal(test, y_test))\n",
    "accuracy = correct_answers / (len(test)*1.0) * 100\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have extra time try following the same steps this our tweets data. Data: *./data/twitter_data/testdata.csv*\n",
    "\n",
    "**Note:** This data contain 3 classes, positive, negative and neutral, you can extract positive and negative tweets and work with them. \n",
    "\n",
    "Here is the data format:\n",
    "* The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "* The id of the tweet (2087)\n",
    "* The date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "* The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "* The user that tweeted (robotickilldozr)\n",
    "* The text of the tweet (Lyx is cool)\n",
    "\n",
    "Also make sure that you read correctly the files, those are csv files that might be easily read using the function _read_csv_ from pandas. Maybe you can compare both read-file-functions and note the advantages of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_file = \"../data/text_twitter/testdata.csv\"\n",
    "tweets_train = open(tweet_file, \"r\").readlines()\n",
    "train_data = pd.read_csv(tweet_file, header=None, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract only text and labels. You might also consider deleting punctuation and weird stuff that might misslead your model.\n",
    "\n",
    "**Hint:** Check the _iterrows_ function in pandas to iterate over each row in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "corpus = []\n",
    "for index, row in train_data.iterrows():\n",
    "    if row[0] == 0 or row[0] == 4:\n",
    "        labels.append(row[0])\n",
    "        cleanText = re.sub(r\"[^A-Za-z0-9]\", \" \", row[5])\n",
    "        cleanText = re.sub(r\"\\s{2,}\", \" \", cleanText)\n",
    "        corpus.append(cleanText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is your time to feed this data again into a model and check it's performance :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
