{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using Neural Networks\n",
    "\n",
    "After having the chance to try different parameters in the Tensorflow Playground, now it's our turn to implement something by ourselves using neural networks. As you may remember we already worked with the Fashion-MNIST dataset using unsupervised methods. Today we're going to use Keras in order to build our model, this time supervised classification. Keras is a high level framework for machine learning, which uses Tensorflow as backend. It allows us to implement neural network in a very confortable form. For more information about Keras go to <https://keras.io/>\n",
    "\n",
    "\n",
    "## Exporing the Data\n",
    "\n",
    "We'll use the same data as for clustering. However, for this exercise we need training and testing samples, so that we can test how well our model performs. Test data is useful to observe that our model is not only memorizing the samples, but it should be able to classify unseen data. Therefore, we don't provide the model with labels in the test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the lib\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "\n",
    "pd.set_option('display.height', 800)\n",
    "pd.set_option('display.width', 800)\n",
    "pd.set_option('max_rows', 20)\n",
    "pd.set_option('max_columns', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are already familiar with the load_data function, it returns train and test data in tuples.\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()\n",
    "\n",
    "print (\"let's have a look in the data:\")\n",
    "print (\"shape of x_train_orig: {}\".format(x_train_orig.shape))\n",
    "print (\"shape of y_train_orig: {}\".format(y_train_orig.shape))\n",
    "print (\"shape of x_test_orig: {}\".format(x_test_orig.shape))\n",
    "print (\"shape of y_test_orig: {}\".format(y_test_orig.shape))\n",
    "print (\"\")\n",
    "\n",
    "(m, w, h) = x_train_orig.shape\n",
    "\n",
    "print (\"x_train_orig is a matrix of shape {},\".format((m, w, h))) \n",
    "print (\"there are {} images, each of them is a 2-dimensional matrix of shape {} (grey-scaled pixels).\".format(m, (w,h)))\n",
    "print (\"\")\n",
    "\n",
    "print (\"y_train_orig is an one-dimensional vector with length {}.\".format(y_train_orig.shape[0]))\n",
    "print (\"Notice: the length of y_train_orig is same with x_train_orig (first dimension).\")\n",
    "print (\"\")\n",
    "\n",
    "i = 42\n",
    "print (\"Now let's peek into {}-th example of the x_train_orig and y_train_orig.\".format(i))\n",
    "print (\"x_train_orig[{}] is a 2D matrix, it looks like\".format(i))\n",
    "print (DataFrame (x_train_orig[i]))\n",
    "\n",
    "print (\"the label for x_train_orig[{}] is y_train_orig[{}] = {}\".format(i, i, y_train_orig[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "As before, we flatten the 2D data into a string of single values.\n",
    "But we also have to pre-process the labels. The learning process does not expect class indices (1,2, ..., 9) but one-hot vectors. One-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). One-hot encoding represents the categorial variable with its dimension being set to value 1:\n",
    "\n",
    "|label|One-hot vector|\n",
    "|-|-|\n",
    "|0|[1,0,0,0,0,0,0,0,0,0] |\n",
    "|1|[0,1,0,0,0,0,0,0,0,0] |\n",
    "|2|[0,0,1,0,0,0,0,0,0,0] |\n",
    "|3|[0,0,0,1,0,0,0,0,0,0] |\n",
    "|4|[0,0,0,0,1,0,0,0,0,0] |\n",
    "|5|[0,0,0,0,0,1,0,0,0,0] |\n",
    "|6|[0,0,0,0,0,0,1,0,0,0] |\n",
    "|7|[0,0,0,0,0,0,0,1,0,0] |\n",
    "|8|[0,0,0,0,0,0,0,0,1,0] |\n",
    "|9|[0,0,0,0,0,0,0,0,0,1] |\n",
    "\n",
    "The separate dimensions provide a more meaningful error value (think about this for a second or two!) and are easy to generalize. Additionally, it is preferable to normalize the image pixel values from a range in [0,255] to a range in [0,1].\n",
    "\n",
    "**Your Assignment** in the following code cell:\n",
    "- Normalize pixel values in the data set x_train_orig with value range [0,255] to x_train with value range [0,1]\n",
    "- Flatten the data image in x_train_orig, x_test_orig 2D to 1D, and store them into x_train and x_test.\n",
    "* Convert the labels in y_train_orig, y_test_orig using one-hot encoding, and store them into x_train and x_test\n",
    "\n",
    "**Hint:**\n",
    "- Normalization: $x'=\\frac{x - min(x)}{max(x) - min(x)}$ where $x$ is an original value, $x'$ is the normalized value\n",
    "    - example: a grey-scaled value $x = 190$, $min(x) = 0$, $max(x) = 255$, then $x'=\\frac{190}{255}$ \n",
    "    - you may apply this operation element-wise on the matrix that you want to normalize\n",
    "- Flatten: \n",
    "    - x_train_orig: [60000,28,28] => x_train: [60000,28\\*28] \n",
    "    - x_test_orig:  [10000,28,28] => x_test: [10000,28\\*28]\n",
    "    - you might find the numpy function `reshape` useful\n",
    "- one-hot encoding\n",
    "    - `keras.utils.np_utils` contains a function that transforms labels to one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# Number of categories in our data \n",
    "num_class = 10\n",
    "\n",
    "# Normalizing pixel values of x_train_orig and x_test_orig, store into x_train and x_test\n",
    "x_train = x_train_orig / 255 # your code\n",
    "x_test  = x_test_orig / 255 # your code\n",
    "\n",
    "# Flatten the data images into one dimentional vectors (60000,28,28) => (60000,28*28) = (60000,784)\n",
    "x_train = np.reshape(x_train, (60000, 28*28)) # your code\n",
    "x_test  = np.reshape(x_test, (10000, 28*28)) # your code\n",
    "\n",
    "# convert the labels in y_train_orig and y_train_test using one-hot encoding (category 10)\n",
    "y_train = np_utils.to_categorical(y_train_orig) # your code\n",
    "y_test  = np_utils.to_categorical(y_test_orig) # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"x_train.shape = {}, expected = {}\".format(x_train.shape, (60000, 28*28)))\n",
    "print (\"y_train.shape = {}, expected = {}\".format(y_train.shape, (60000, 10)))\n",
    "print ()\n",
    "\n",
    "print (\"x_test.shape = {}, expected = {}\".format(x_test.shape, (10000, 28*28)))\n",
    "print (\"y_test.shape = {}, expected = {}\".format(y_test.shape, (10000, 10)))\n",
    "print ()\n",
    "\n",
    "# examine some example\n",
    "r = np.random.randint(60000)\n",
    "print(\"Now, let's examine {}-th examplar of the normalized x_train.\".format(r))\n",
    "print(\"x_train[{}] looks like:\".format(r))\n",
    "print(DataFrame(x_train[r]))\n",
    "\n",
    "print(\"The label of x_train[{}]: y_train[{}] looks like\".format(r, r))\n",
    "print(DataFrame(y_train[r]))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model\n",
    "\n",
    "Now we have to define the structure of the neural network. <br>\n",
    "For another step we import the TensorBoard library, so that we can visualize our results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "We are going to use [keras sequential model](https://keras.io/getting-started/sequential-model-guide/) to build a sequential neural network.\n",
    "\n",
    "**Outline of the model**\n",
    "![funct](../data/nn-model.png)\n",
    "The diagram shows a network with 2 fully (densely) connected hidden layers. \n",
    "<br>\n",
    "\n",
    "**Input**\n",
    "- The input size has to be the size of a flattened image (784)\n",
    "<br>\n",
    "\n",
    "**Hidden layers**\n",
    "- The size of the hidden layers can be choosen as you like, we propose:\n",
    "  - the first layer with 128 neurons, using `ReLU`as activation function\n",
    "  - the second layer with 64 neurons, using `ReLU` as activation function\n",
    "<br>\n",
    "\n",
    "**Output**\n",
    "- the output layer again has to be equal to the number of classes (10), using `softmax` as activation function\n",
    "\n",
    "**Your assignment**:\n",
    "- implement the model in diagram above using keras sequential model\n",
    "    - define a sequential model\n",
    "    - define first layer: size 128, activation=ReLU\n",
    "    - define second layer: size 64, activation=ReLU\n",
    "    - define output layer: size 10, activation=Softmax\n",
    "\n",
    "**Hint**\n",
    "- Use `Sequential` to declare a new sequential model\n",
    "    - After you have a sequential `model`, your can use `model.add()` to add a layer\n",
    "- Use `Dense` to define a layer\n",
    "    - you need to provide parameter `input_shape` at the first layer \n",
    "- Use `Activation` to define activation\n",
    "- Read: [Guide to sequential model](https://keras.io/getting-started/sequential-model-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "import keras\n",
    "\n",
    "# declare one sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add first layer, size 128, input shape = size of a flattened image, use ReLU as activation\n",
    "model.add(Dense(128, input_shape=x_train[0].shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# add second layer siez 64, use ReLU as activation\n",
    "model.add(Dense(64)) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# add output layer, size 10 (number of classes), use Softmax as activation\n",
    "model.add(Dense(num_class)) \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Generates a graph event to visualize the control flow.\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "# Summarizes the settings and outputs the complexity of our model.\n",
    "# In other words, how many weights have influence to the output.\n",
    "# The more degrees of freedom, the more labeled data should be present.\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "**Your Assignment**\n",
    "- train the model with the train data, labels, and test data. \n",
    "\n",
    "**Hint**\n",
    "\n",
    "The training itself is nothing special. We use the method `model.fit` and define the relevant data: \n",
    "- the data (`x`=$x\\_train$)\n",
    "- the labels as one-hot vectors (`y`=$y\\_train$)\n",
    "- the number of iterations(`epochs`)\n",
    "- the batch size(`batch_size`)\n",
    "- optional: the validation data (`validation_data`=($x\\_test$, $y\\_test$))\n",
    "- optional: the callbacks (`callbacks`=[tensorboard])\n",
    "\n",
    "If you enter the test data as `validation_data`, we get the calculated model quality after each epoch on the basis of the test data.\n",
    "\n",
    "`Epochs` are iterations over all data points. The less data we have the more we have to iterate to improve the weights often enough. With more epochs the learning process receives the same data multiple times. There is a risk that the model memorizes the patterns and doesn't generalize any more. This is called *overfitting*.\n",
    "The `batch_size` defines the number of instances, whose error is examined by the optimizer before the weights will be adapted. \n",
    "\n",
    "The fit-method delivers the history, which allows us to visualize the training process. Furthermore this method includes a `callbacks` attribute, it is fed with the tensorboard object and enables us an access to the Tensorboard.\n",
    "\n",
    "**Read**: method `fit` in [sequential document](https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 3\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Progress of accuracy \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training accuracy increases during the epochs, because the model adapts to the training data. \n",
    "At some point in the training this is not the case anymore for the test data and it could become even worse. \n",
    "So we have to consider not overtraining the model (*overfitting*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "Another visualisation method is called *Tensorboard*. To understand, debug, and optimize your model the Tensorbaord includes some visualization tools. You can inspect your computation graph, or plot quantitative metrics like the accuracy or the loss function.\n",
    "\n",
    "To open Tensorboard you have to proceed the following way (**see next cell!**):\n",
    "\n",
    "- Start Tensorboard in your log directory: \n",
    "\n",
    "    `tensorboard --logdir=<path_to_log_folder>`\n",
    "  \n",
    "  \n",
    "- Open port 6006 in your browser (copy and paste): \n",
    "\n",
    "(***replace <your_docker_ip> by your own docker-ip***)\n",
    "\n",
    "    `http://<your_docker_ip>:6006`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/home/jovyan/exercises/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The information during the training was already promising. As in previous exercises, let's take a look at the confusion matrix to estimate the numbers. In order to do so, we use our trained model and let it make predict on the test data.\n",
    "\n",
    "**Your Assignment**\n",
    "- use the model to predict on test data\n",
    "- calculate the confusion matrix and accuracy\n",
    "\n",
    "***Hint:***\n",
    "- `model.predict_classes` returns the labels directly, saving the conversion of one-hot vectors.\n",
    "    - *Note*: the result will be categorial values, not one-hot encoded value.\n",
    "- use `metrics` from sklearn to calculate the confusion matrix and accuracy\n",
    "    - in order to compare with the predictions, you should use `y_test_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# predict on test data\n",
    "predictions = model.predict_classes(x_test);\n",
    "\n",
    "# confusion matrix and accuracy\n",
    "cm = metrics.confusion_matrix(y_test_orig, predictions)\n",
    "accuracy = metrics.accuracy_score(y_test_orig, predictions)\n",
    "\n",
    "# Output\n",
    "print(\"ACC: {}\".format(accuracy))\n",
    "print(\"CM:\")\n",
    "print(DataFrame(cm))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks awesome! Let's save the model so that we can use it again at any time without any effort. New data can now be preprocessed in the same way and classified using `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('MyFashionClassifier.h5') # Save the current status in a HDF5 format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
