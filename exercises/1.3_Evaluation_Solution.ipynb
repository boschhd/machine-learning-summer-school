{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluation\n",
    "\n",
    "Machine learning always tries to generate 'artifacts' (eg. predictions, decissions, clusters, ...).\n",
    "Evaluation can estimate the quality of a  model based on the generated artifacts with two fundamental principles:\n",
    " - If the expected, correct outcome is known we can calculate the **error**\n",
    " - If the expected outcome is not known, we formularize the \"uglyness\" of the artifact, usually called the **cost function** or **stress**\n",
    "\n",
    "Both evaluation measures are often called the **score**\n",
    "\n",
    "## Error based scores\n",
    "\n",
    "### Regression\n",
    "\n",
    "Regression analysis estimates the relationships among variables. It tries to model the relationship between a dependent variable (the 'target' or 'criterion') and one or more independent variables (the 'features' or 'predictors'). This allows to predict a target value based on the given feature values.\n",
    "\n",
    "Calculating the regression error is rather simple. Try to summarize the differences between prediction and correct value.\n",
    "\n",
    "_Let's generate and visualize some data:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# generate this many points\n",
    "numPoints = 42;\n",
    "rng = np.random.RandomState()\n",
    "\n",
    "# these variables are actually vectors of the size numPoints\n",
    "x = 10 * rng.rand(numPoints) \n",
    "y = 2 * x - 1 + rng.randn(numPoints)\n",
    "\n",
    "# how does the data look like?\n",
    "print('X: {}'.format(x))\n",
    "print('Y: {}'.format(y))\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's next?\n",
    "\n",
    "In order to calculate an error, we first have to make some predictions. Lets start with a most basic statistical prediction, aptly named \"Expectation\".\n",
    "\n",
    "1. Calcualate the average of the y values.\n",
    "2. Our regression model is then to always return the mean if someone asks for a prediction based on a x value\n",
    "3. Plot the data and the regression\n",
    "4. Calculate the standard deviation as an error metric and print it\n",
    "\n",
    "\n",
    "***Hints:***\n",
    "\n",
    "Mean = $\\frac{\\sum_{i}^{n} x_i}{n}$\n",
    "\n",
    "Standard Deviation = $\\sqrt{\\frac{\\sum_{i}^{n} (\\check{y_i }- y_i)^2}{n-1}}$\n",
    "\n",
    "np.full(n,value) creates a vector of size n which has the same value at every position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.sum(y)/numPoints\n",
    "\n",
    "# Option one. make a vector of mean values the same size as the data\n",
    "y_reg = np.full(numPoints, mean);\n",
    "plt.plot(x,y_reg,'g')\n",
    "\n",
    "# Option two, draw a line \n",
    "plt.axhline(y=mean, color='r')\n",
    "plt.scatter(x, y)\n",
    "print(\"Mean: {}\".format(mean))\n",
    "\n",
    "# error calculation\n",
    "error = np.sqrt(np.sum(np.square(y-mean))/(numPoints-1))\n",
    "print(\"Error: {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What' next?\n",
    "\n",
    "In order to put this error value in perspective we could compare it with better and worse prediction models. Try to formulate these, plot them and calculate their error values\n",
    "\n",
    "***Hints:***\n",
    "- Take the formula of the data generation in the first cell (without the randomness!) as very good model\n",
    "- Take some kind of inverse of the formula as a worse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "\n",
    "\n",
    "y_reg_good = 2*x\n",
    "plt.plot(x,y_reg_good,'g')\n",
    "error = np.sqrt(np.sum(np.square(y-y_reg_good))/numPoints)\n",
    "print(\"Error of good model: {}\".format(error))\n",
    "\n",
    "y_reg_bad = 20-2*x\n",
    "plt.plot(x,y_reg_bad, 'r')\n",
    "error = np.sqrt(np.sum(np.square(y-y_reg_bad))/numPoints)\n",
    "print(\"Error of bad model: {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Classification tries to assign a class instead of a skalar value. The result is often a binary decision and therefore the error is not quantifiable as a difference but every prediction is either correct or wrong.\n",
    "If we compare the prediction with the truth, there are four possible cases (see slides) on wich most other evaluation scores are based on.\n",
    "\n",
    "_Let's generate and visualize some data:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "numPoints = 100\n",
    "lexicon = [\"hund\", \"katze\", \"maus\"]\n",
    "\n",
    "# init with zeroes set randomly one class true for each \"point\"\n",
    "truth       = np.zeros((numPoints,3), dtype=int)\n",
    "predictions = np.zeros((numPoints,3), dtype=int)\n",
    "for i in range(numPoints):\n",
    "    index = random.randint(0,2)\n",
    "    truth[i][index] = 1\n",
    "    predictions[i][((index+int(random.gauss(0,1.2))) % 3)] = 1\n",
    "\n",
    "# visualize\n",
    "order = np.arange(len(lexicon))\n",
    "values_truth = np.sum(truth,axis=0).tolist()\n",
    "values_preds = np.sum(predictions,axis=0).tolist()\n",
    "\n",
    "plt.bar(order, values_truth)\n",
    "plt.xticks(order, lexicon)\n",
    "plt.ylabel('Count')\n",
    "plt.title('truth')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(order, values_preds)\n",
    "plt.xticks(order, lexicon)\n",
    "plt.ylabel('Count')\n",
    "plt.title('predictions')\n",
    "plt.show()\n",
    "\n",
    "CEND      = '\\33[0m'\n",
    "CRED    = '\\33[31m'\n",
    "CGREEN  = '\\33[32m'\n",
    "for i in range(numPoints):\n",
    "    if np.dot(truth[i],predictions[i]):\n",
    "        print(CGREEN + \"Row: {} T: {} P: {}\".format(i,truth[i],predictions[i],) + CEND, end=\", \")\n",
    "    else:\n",
    "        print(CRED + \"Row: {} T: {} P: {}\".format(i,truth[i],predictions[i],)+ CEND, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next\n",
    "\n",
    "If you have more the two classes, the generalization of the four possible outcome cases is the *confusion matrix*. You can calculate it by creating an empty *n x n* matrix, where *n* are the number of classes, and then going through each prediction of the test set increase the cell counts accordingly.\n",
    "\n",
    "***Hints:***\n",
    "- dot products might be of help (np.dot())\n",
    "- scitkit-learn also has a method to calucalte the confusion matrix, but you have to transform into the expected format first.\n",
    "- To visualize the matrix you can use mathplots matshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 by hand;\n",
    "# init empty matrix with the correct size\n",
    "cm = np.zeros((len(lexicon),len(lexicon)), dtype=int)\n",
    "# count cases\n",
    "for i in range(numPoints):\n",
    "    index_truth = np.dot(truth[i],[0,1,2])\n",
    "    index_preds = np.dot(predictions[i],[0,1,2])\n",
    "    cm[index_truth][index_preds] += 1\n",
    "\n",
    "print(cm)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm, cmap=plt.cm.gray)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticklabels(['']+lexicon)\n",
    "ax.set_yticklabels(['']+lexicon)\n",
    "\n",
    "\n",
    "#plt.matshow(confusion, cmap=plt.cm.gray)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 2 with scikit-learn\n",
    "\n",
    "# transform one-hot-vectors to class indices ([0,0,1],[1,0,0],[0,1,0] -> 2,0,1)\n",
    "y_truth_index = [ np.argmax(t) for t in truth ]\n",
    "y_predi_index = [ np.argmax(t) for t in predictions ]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "print(sk_cm(y_truth_index, y_predi_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "Now we can calcualte the derived evaluation metrix. We supplied a function which takes the confusion matrix and a class index (0=hund, 1=katze, 2=maus) and calculates the true/false positives/negatives counts for this class. You can use these values to calculate Accuracy, Recall, Precision, F-1 Score for each class individually.\n",
    "\n",
    "Think about how your predictor could cheat and produce optimal Recall or optimal Precission easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cases(class_index):\n",
    "    i = class_index\n",
    "    mask = mask = np.identity(3)\n",
    "    mask[i][i] = 0;\n",
    "    return {\n",
    "        'TP': cm[i][i],\n",
    "        'TN': (np.sum(np.dot(mask,np.dot(cm,mask)))),\n",
    "        'FN': (np.sum(cm[i])-cm[i][i]),\n",
    "        'FP': (np.sum(cm, axis=0)[i]-cm[i][i])\n",
    "    }\n",
    "\n",
    "for i in range(len(lexicon)):\n",
    "    cases = get_cases(i)\n",
    "    print(\"Class: {} Cases: {} -> \".format(lexicon[i],cases))\n",
    "    print(\"Accuracy:  {0:.0%}\".format( (cases['TP']+cases['TN'])/ numPoints ))\n",
    "    print(\"Recall:    {0:.0%}\".format( (cases['TP'])            / (cases['TP']+cases['FN']) ))\n",
    "    print(\"Precision: {0:.0%}\".format( (cases['TP'])            / (cases['TP']+cases['FP']) ))\n",
    "\n",
    "    recall = (cases['TP'])/(cases['TP']+cases['FN'])\n",
    "    precis = (cases['TP'])/(cases['TP']+cases['FP'])\n",
    "\n",
    "    print(\"F1 Score : {0:.0%}\".format(2*recall*precis/(recall+precis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress-based Evaluation\n",
    "\n",
    "We will defer this topic until we have learned more about clustering :)\n",
    "\n",
    "But for completeness, in a nutshell: There is no ground truth but only desired qualities such as:\n",
    "- good between-cluster separation (large distance)\n",
    "- good within-cluster connectivity (small distance)\n",
    "- and many more based on the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# number of clusters to generate and to seek\n",
    "k = 4;\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=k,\n",
    "                       cluster_std=0.20, random_state=0)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Divide the sum of distances throug the number of distances\n",
    "avg_separation = np.sum(pairwise_distances(centers))/k/(k-1)\n",
    "\n",
    "# do the same for the members of each cluster \n",
    "# create an array of sets\n",
    "groups = []\n",
    "for i in range(k):\n",
    "    groups.append([]); \n",
    "# assign the members to their groups\n",
    "for i in range(len(X)):\n",
    "  groups[y_kmeans[i]].append(X[i])\n",
    "\n",
    "connectivity = []\n",
    "for i in range(k):\n",
    "  connectivity.append(np.sum(pairwise_distances(groups[i]))/len(groups[i])/(len(groups[i])-1))\n",
    "\n",
    "# print results\n",
    "print(\"Average separation: {}\".format(avg_separation))\n",
    "print(\"Average connectivity: {} ({})\".format(np.average(connectivity),connectivity))\n",
    "\n",
    "print(\"Ratio separation / connectivity: {} (higher is better)\".format(avg_separation/np.average(connectivity)))\n",
    "\n",
    "# visual \n",
    "order = range(3)\n",
    "values = ()\n",
    "values_preds = np.sum(predictions,axis=0).tolist()\n",
    "\n",
    "plt.bar(range(3), [avg_separation,np.average(connectivity),avg_separation/np.average(connectivity)])\n",
    "plt.xticks(range(3), [\"Sep\", \"Con\", \"Ratio\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "You can now play with the data generation parameter *cluster_std* and see how the evaluation metric changes. Higher STDs should create worse results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
